# -*- coding: utf-8 -*-
"""distilbert_finetuning_for_news_topic_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ps-rYUYlWSiegWWFjvM0qcV2EJawY5j-

# Install Libraries
"""

try:
  import torch
except:
  !pip install torch
  import torch

try:
  import transformers
except:
  !pip install transformers
  import transformers

try:
  import datasets
except:
  !pip install datasets
  import datasets

try:
  import sklearn
except:
  !pip install sklearn
  import sklearn

"""# Load Tokenizer"""

from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

"""#Load and preprocess data"""

from transformers import DataCollatorWithPadding

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True)

dataset = datasets.load_dataset("ag_news")

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

train_dataset = tokenized_datasets["train"]
val_dataset = tokenized_datasets["test"]

data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)

"""# Load Model"""

from transformers import AutoModelForSequenceClassification

num_labels = len(train_dataset.unique("labels"))
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)

"""#Training"""

def compute_metrics(p):
    predictions, labels = p
    preds = predictions.argmax(axis=1)
    return {"accuracy": sklearn.metrics.accuracy_score(labels, preds)}

training_args = transformers.TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    # learning_rate=2e-5,
    # per_device_train_batch_size=8,
    # per_device_eval_batch_size=8,
    # num_train_epochs=3,
    # weight_decay=0.01,
    report_to=["none"],
)

trainer = transformers.Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

trainer.train()

"""# Model Evaluation and Predictions"""

results = trainer.evaluate()
print(f"Accuracy: {results['eval_accuracy']}")

text = "The new Apple product is very good"
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

for key, value in inputs.items():
    inputs[key] = value.to(model.device)

outputs = model(**inputs)
logits = outputs.logits
predicted_class = logits.argmax().item()

label_map = {
    0: "World",
    1: "Sports",
    2: "Business",
    3: "Science/Tech"
}

print(f"Input: \"{text}\", Predicted Class: {label_map[predicted_class]}")

"""# Save Model"""

model.save_pretrained("./fine_tuned_distilbert")
tokenizer.save_pretrained("./fine_tuned_distilbert")

!git add fine_tuned_distilbert/

